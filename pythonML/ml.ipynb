{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d30e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall torch==1.13.1+cu117 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84ac688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 16:48:36.226365: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-07 16:48:36.226421: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-07 16:48:36.226463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-07 16:48:36.235993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import constants\n",
    "# from datasets import load_dataset\n",
    "# from utils import clean, make_current_datetime_dir, compute_metrics, preprocess_data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e19600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
      "CUDA_VISIBLE_DEVICES: 5\n",
      "1.13.1+cu117\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "# CUDA_DEVICE_ORDER 및 CUDA_VISIBLE_DEVICES 환경 변수 확인\n",
    "cuda_order = os.getenv(\"CUDA_DEVICE_ORDER\")\n",
    "cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")\n",
    "\n",
    "print(\"CUDA_DEVICE_ORDER:\", cuda_order)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", cuda_visible_devices)\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719a9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "True\n",
      "0\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b1909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = 'mydata/'\n",
    "train_path = mydata+'mydata_train.csv'\n",
    "test_path = mydata+'mydata_test.csv'\n",
    "valid_path = mydata+'mydata_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1db4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv(train_path, sep=',')\n",
    "test_data = pd.read_csv(test_path, sep=',')\n",
    "valid_data = pd.read_csv(valid_path, sep=',')\n",
    "\n",
    "# 컬럼 이름 설정 (예시)\n",
    "train_data.columns = ['text', 'label']\n",
    "test_data.columns = ['text', 'label']\n",
    "valid_data.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0636b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train_data = preprocess_data(train_data)\n",
    "# unq_labels, cnt_labels = np.unique(processed_train_data['label'].values, return_counts=True)\n",
    "# print('labels =', dict(zip(unq_labels, cnt_labels)))\n",
    "# filtered_data = processed_train_data[processed_train_data['label'] == 0]\n",
    "# filtered_data\n",
    "# processed_train_data\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce59ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('beomi/KcELECTRA-base-v2022', num_labels=3)\n",
    "model = model.to(device)\n",
    "# print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8f17e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '자한당', '##틀딱', '##들', '.', '.', '악플', '##질', '고만해라', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[2, 8535, 15271, 4079, 18, 18, 10377, 4077, 27100, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "디코딩 : [CLS] 자한당틀딱들.. 악플질 고만해라. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "max_len = 64          # 문장의 길이 (평균 13정도)\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "log_interval = 500    # metrics 생성 시점\n",
    "\n",
    "# 학습 데이터\n",
    "# ==> 토크나이징 + 패딩 : dim=64\n",
    "encoded_train = tokenizer(\n",
    "    train_data['text'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    max_length=max_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# 확인\n",
    "print( encoded_train[0].tokens )\n",
    "print( encoded_train[0].ids )\n",
    "print( encoded_train[0].attention_mask )\n",
    "print()\n",
    "print('디코딩 :',tokenizer.decode(encoded_train[0].ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ccc08dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩 : [CLS] 서로만이 이렇게 있을때 보기 좋다♥ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "디코딩 : [CLS] 아근데 진짜 영화가 너무 웃김 ㅋㅋㅋㅋㅋㅋ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터\n",
    "# ==> 토크나이징 + 패딩 : dim=64\n",
    "encoded_valid = tokenizer(\n",
    "    valid_data['text'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    max_length=max_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print('디코딩 :',tokenizer.decode(encoded_valid[0].ids))\n",
    "print('디코딩 :',tokenizer.decode(encoded_valid[-1].ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693674d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78977,) int64\n",
      "(8776,) int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작 전에 label 데이터가 int형에 1차원인지 확인\n",
    "\n",
    "print(train_data['label'].values.shape, train_data['label'].values.dtype)\n",
    "print(valid_data['label'].values.shape, valid_data['label'].values.dtype, end='\\n\\n')\n",
    "train_data['label'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f69ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoelectraDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9936839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "train_dataset = KoelectraDataset(encoded_train, train_data['label'].values)\n",
    "valid_dataset = KoelectraDataset(encoded_valid, valid_data['label'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf559c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 측정을 위한 함수 정의\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # average: 'micro', 'macro', 'weighted' or 'samples' \n",
    "    # 참고 https://aimb.tistory.com/152\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f9efff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 성능 시각화 wandb\n",
    "# https://docs.wandb.ai/guides/integrations/huggingface\n",
    "\n",
    "# 학습 파라미터\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./temp/electra',\n",
    "    overwrite_output_dir='True',\n",
    "\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    " \n",
    "    logging_dir='./temp/logs',\n",
    "    logging_steps=log_interval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=log_interval,\n",
    "\n",
    "    # https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442/8\n",
    "    save_total_limit=2,\n",
    "    save_strategy='no',\n",
    "    load_best_model_at_end=False,\n",
    "    \n",
    "    warmup_ratio=0.1\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2f7119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4938' max='4938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4938/4938 12:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.576800</td>\n",
       "      <td>0.372312</td>\n",
       "      <td>0.865428</td>\n",
       "      <td>0.867787</td>\n",
       "      <td>0.877032</td>\n",
       "      <td>0.865428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.298580</td>\n",
       "      <td>0.890269</td>\n",
       "      <td>0.889518</td>\n",
       "      <td>0.889639</td>\n",
       "      <td>0.890269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.325407</td>\n",
       "      <td>0.882293</td>\n",
       "      <td>0.883476</td>\n",
       "      <td>0.889557</td>\n",
       "      <td>0.882293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261171</td>\n",
       "      <td>0.904740</td>\n",
       "      <td>0.904584</td>\n",
       "      <td>0.904483</td>\n",
       "      <td>0.904740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.294268</td>\n",
       "      <td>0.903373</td>\n",
       "      <td>0.902853</td>\n",
       "      <td>0.902832</td>\n",
       "      <td>0.903373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.279909</td>\n",
       "      <td>0.904512</td>\n",
       "      <td>0.904577</td>\n",
       "      <td>0.904708</td>\n",
       "      <td>0.904512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.298261</td>\n",
       "      <td>0.908387</td>\n",
       "      <td>0.908326</td>\n",
       "      <td>0.908283</td>\n",
       "      <td>0.908387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.290942</td>\n",
       "      <td>0.908273</td>\n",
       "      <td>0.908426</td>\n",
       "      <td>0.908661</td>\n",
       "      <td>0.908273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.284275</td>\n",
       "      <td>0.910665</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.910827</td>\n",
       "      <td>0.910665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4938, training_loss=0.26043323410238123, metrics={'train_runtime': 753.4933, 'train_samples_per_second': 209.629, 'train_steps_per_second': 6.553, 'total_flos': 5194977097976064.0, 'train_loss': 0.26043323410238123, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeae91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
